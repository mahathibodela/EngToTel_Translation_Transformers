{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2119948,
          "sourceType": "datasetVersion",
          "datasetId": 1272055
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "translation",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'samanantar:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1272055%2F2119948%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240319%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240319T055216Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db85fd6ad1b47458687401bbb352499aa55dce605eed7180a602266bcd036028bbd2be47b7dad90248cf17ecfaa37fe2b278340511fd118b412b8dd3aaabd88e48bf45f077a89877d7d02d7e27145c3e6ecdd26fcfb46f85117e67e311dab40dd36e734e2c1300ca93021019b70691b54613a4d3b462a9dd1232a18ec84e4e985764588644539c8b3a7080bda082e195776c2a777b1b09deb6abd9f569280a06df5a0e0b6caa33787c8ab34d2809b343db2f1ae53cc532a86265dac01f6e195811a17bd2132a38b6e6fc2c211c028be382e2761fe2a0d4764f39ccdaf93a95245a2dbbf844314530c138088d20f25376c0d436d452a8d75535fa1b8d995d0ddda'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "r34ufoa9RI31"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n014Yh1gRI34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-19T03:49:56.352434Z",
          "iopub.execute_input": "2024-03-19T03:49:56.353301Z",
          "iopub.status.idle": "2024-03-19T03:49:57.731011Z",
          "shell.execute_reply.started": "2024-03-19T03:49:56.35326Z",
          "shell.execute_reply": "2024-03-19T03:49:57.730099Z"
        },
        "trusted": true,
        "id": "iAxm-I1tRI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import torch.nn.functional as f"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:49:57.732709Z",
          "iopub.execute_input": "2024-03-19T03:49:57.73378Z",
          "iopub.status.idle": "2024-03-19T03:50:02.064158Z",
          "shell.execute_reply.started": "2024-03-19T03:49:57.733745Z",
          "shell.execute_reply": "2024-03-19T03:50:02.063015Z"
        },
        "trusted": true,
        "id": "S24Hv-9sRI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_file = \"/kaggle/input/samanantar/final_data/en-te/train.en\"\n",
        "telugu_file = \"/kaggle/input/samanantar/final_data/en-te/train.te\"\n",
        "\n",
        "START_TOKEN = \"<START>\"\n",
        "PADDING_TOKEN = \"<PADDING>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "\n",
        "english_vocab = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R',\n",
        "                        'S','T','U','V','W','X','Y','Z',\n",
        "                        '[', '\\\\', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "telugu_vocab = [ START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                ':', '<', '=', '>', '?', '@',\n",
        "                '[', '\\\\', ']', '^', '_', '`','{', '|', '}', '~',\n",
        "                'े', '्', '॥', 'ఁ', 'ం', 'ః', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ఌ',\n",
        "                'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'క', 'ఖ', 'గ', 'ఘ', 'ఙ', 'చ', 'ఛ', 'జ', 'ఝ',\n",
        "                'ఞ', 'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ',\n",
        "                'మ', 'య', 'ర', 'ఱ', 'ల', 'ళ', 'వ', 'శ', 'ష', 'స', 'హ', 'ా', 'ి', 'ీ', 'ు',\n",
        "                'ూ', 'ృ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', '్', 'ౖ', 'ౘ', 'ౙ', 'ౠ', 'ౡ',\n",
        "                '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',\n",
        "                PADDING_TOKEN, END_TOKEN\n",
        "               ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:02.065414Z",
          "iopub.execute_input": "2024-03-19T03:50:02.066395Z",
          "iopub.status.idle": "2024-03-19T03:50:02.084245Z",
          "shell.execute_reply.started": "2024-03-19T03:50:02.066334Z",
          "shell.execute_reply": "2024-03-19T03:50:02.082832Z"
        },
        "trusted": true,
        "id": "oRotru6hRI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"కృష్ణ\"\n",
        "list(text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:02.087623Z",
          "iopub.execute_input": "2024-03-19T03:50:02.089431Z",
          "iopub.status.idle": "2024-03-19T03:50:02.102829Z",
          "shell.execute_reply.started": "2024-03-19T03:50:02.08939Z",
          "shell.execute_reply": "2024-03-19T03:50:02.10189Z"
        },
        "trusted": true,
        "id": "aB8kR7o8RI36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'క' +  'ె'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:02.104135Z",
          "iopub.execute_input": "2024-03-19T03:50:02.104756Z",
          "iopub.status.idle": "2024-03-19T03:50:02.121363Z",
          "shell.execute_reply.started": "2024-03-19T03:50:02.104723Z",
          "shell.execute_reply": "2024-03-19T03:50:02.119948Z"
        },
        "trusted": true,
        "id": "PLRmkPLZRI36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_telugu = {ind:te for ind, te in enumerate(telugu_vocab)}\n",
        "telugu_to_index = {te:ind for ind, te in enumerate(telugu_vocab)}\n",
        "index_to_english = {ind:en for ind, en in enumerate(english_vocab)}\n",
        "english_to_index = {en:ind for ind, en in enumerate(english_vocab)}\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:02.122864Z",
          "iopub.execute_input": "2024-03-19T03:50:02.123253Z",
          "iopub.status.idle": "2024-03-19T03:50:02.140506Z",
          "shell.execute_reply.started": "2024-03-19T03:50:02.12322Z",
          "shell.execute_reply": "2024-03-19T03:50:02.139256Z"
        },
        "trusted": true,
        "id": "-ZwMSzo5RI36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(english_file, 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "with open(telugu_file, 'r') as file:\n",
        "    telugu_sentences = file.readlines()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:02.142436Z",
          "iopub.execute_input": "2024-03-19T03:50:02.142812Z",
          "iopub.status.idle": "2024-03-19T03:50:22.586793Z",
          "shell.execute_reply.started": "2024-03-19T03:50:02.142781Z",
          "shell.execute_reply": "2024-03-19T03:50:22.585621Z"
        },
        "trusted": true,
        "id": "xhZ7ylkIRI36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(english_sentences))\n",
        "print(len(telugu_sentences))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:22.58827Z",
          "iopub.execute_input": "2024-03-19T03:50:22.588744Z",
          "iopub.status.idle": "2024-03-19T03:50:22.594978Z",
          "shell.execute_reply.started": "2024-03-19T03:50:22.588699Z",
          "shell.execute_reply": "2024-03-19T03:50:22.593981Z"
        },
        "trusted": true,
        "id": "yo9sLtzVRI37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_sentences[:3])\n",
        "print(telugu_sentences[:3])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:22.596189Z",
          "iopub.execute_input": "2024-03-19T03:50:22.597093Z",
          "iopub.status.idle": "2024-03-19T03:50:22.61558Z",
          "shell.execute_reply.started": "2024-03-19T03:50:22.597056Z",
          "shell.execute_reply": "2024-03-19T03:50:22.614361Z"
        },
        "trusted": true,
        "id": "njBgoe8ERI4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOT_SEN = 30000\n",
        "\n",
        "# as sentences ke last pai \\n hain usko nikalna hain\n",
        "# and we dont need to all the sentences to do training\n",
        "english_sentences = [english_sentences[i].rstrip('\\n') for i in range(TOT_SEN)]\n",
        "telugu_sentences = [telugu_sentences[i].rstrip('\\n') for i in range(TOT_SEN)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:22.620497Z",
          "iopub.execute_input": "2024-03-19T03:50:22.620972Z",
          "iopub.status.idle": "2024-03-19T03:50:23.272271Z",
          "shell.execute_reply.started": "2024-03-19T03:50:22.620923Z",
          "shell.execute_reply": "2024-03-19T03:50:23.270812Z"
        },
        "trusted": true,
        "id": "3XeH1p1sRI4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_sentences[:3])\n",
        "print(telugu_sentences[:3])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:23.273684Z",
          "iopub.execute_input": "2024-03-19T03:50:23.27404Z",
          "iopub.status.idle": "2024-03-19T03:50:23.28036Z",
          "shell.execute_reply.started": "2024-03-19T03:50:23.274009Z",
          "shell.execute_reply": "2024-03-19T03:50:23.27908Z"
        },
        "trusted": true,
        "id": "MnporKCgRI4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plan to pass embeddings for each token rather than word\n",
        "max(len(x) for x in english_sentences), max(len(x) for x in telugu_sentences)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:23.282018Z",
          "iopub.execute_input": "2024-03-19T03:50:23.282424Z",
          "iopub.status.idle": "2024-03-19T03:50:23.302413Z",
          "shell.execute_reply.started": "2024-03-19T03:50:23.28236Z",
          "shell.execute_reply": "2024-03-19T03:50:23.300951Z"
        },
        "trusted": true,
        "id": "2Hc1PsnVRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PERCENTAILE = 97\n",
        "# printing the length of sentneces for 97% of the sentences\n",
        "np.percentile([len(x) for x in english_sentences], PERCENTAILE)\n",
        "np.percentile([len(x) for x in telugu_sentences], PERCENTAILE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:23.304112Z",
          "iopub.execute_input": "2024-03-19T03:50:23.304668Z",
          "iopub.status.idle": "2024-03-19T03:50:23.337834Z",
          "shell.execute_reply.started": "2024-03-19T03:50:23.304619Z",
          "shell.execute_reply": "2024-03-19T03:50:23.336344Z"
        },
        "trusted": true,
        "id": "7S-KCLKMRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 200\n",
        "\n",
        "def is_valid_sen(sen, vocab):\n",
        "    for token in sen:\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sen, max_len):\n",
        "    if len(sen) < max_len - 1: # need to add end token along that 200\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "telugu__sentences = []\n",
        "english__sentences = []\n",
        "\n",
        "for i in range(len(telugu_sentences)):\n",
        "    eng_sen, tel_sen = english_sentences[i], telugu_sentences[i]\n",
        "    if is_valid_sen(eng_sen, english_vocab) and is_valid_sen(tel_sen, telugu_vocab) and is_valid_length(eng_sen, MAX_SEQ_LEN) and is_valid_length(tel_sen, MAX_SEQ_LEN):\n",
        "        telugu__sentences.append(tel_sen)\n",
        "        english__sentences.append(eng_sen)\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:23.339279Z",
          "iopub.execute_input": "2024-03-19T03:50:23.339642Z",
          "iopub.status.idle": "2024-03-19T03:50:29.043093Z",
          "shell.execute_reply.started": "2024-03-19T03:50:23.339611Z",
          "shell.execute_reply": "2024-03-19T03:50:29.041774Z"
        },
        "trusted": true,
        "id": "l1nBgw2JRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(telugu__sentences), len(english__sentences)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:29.044997Z",
          "iopub.execute_input": "2024-03-19T03:50:29.045959Z",
          "iopub.status.idle": "2024-03-19T03:50:29.053588Z",
          "shell.execute_reply.started": "2024-03-19T03:50:29.045914Z",
          "shell.execute_reply": "2024-03-19T03:50:29.052303Z"
        },
        "trusted": true,
        "id": "anbX6IPsRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokeniser(sen, lan_to_index):\n",
        "    sen_to_ind = [lan_to_index[x] for x in sen]\n",
        "    sen_to_ind.insert(0, lan_to_index[START_TOKEN])\n",
        "    sen_to_ind.append(lan_to_index[END_TOKEN])\n",
        "    for _ in range(len(sen_to_ind), MAX_SEQ_LEN):\n",
        "        sen_to_ind.append(lan_to_index[PADDING_TOKEN])\n",
        "    return torch.tensor(sen_to_ind)\n",
        "\n",
        "telugu_sen_tokensied = [tokeniser(x, telugu_to_index) for x in telugu__sentences]\n",
        "english_sen_tokensied = [tokeniser(x, english_to_index) for x in english__sentences]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:29.055431Z",
          "iopub.execute_input": "2024-03-19T03:50:29.055958Z",
          "iopub.status.idle": "2024-03-19T03:50:33.769117Z",
          "shell.execute_reply.started": "2024-03-19T03:50:29.055911Z",
          "shell.execute_reply": "2024-03-19T03:50:33.767906Z"
        },
        "trusted": true,
        "id": "zEMn80avRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(telugu_sen_tokensied[:2])\n",
        "print(english_sen_tokensied[:2])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.77065Z",
          "iopub.execute_input": "2024-03-19T03:50:33.770996Z",
          "iopub.status.idle": "2024-03-19T03:50:33.810431Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.770968Z",
          "shell.execute_reply": "2024-03-19T03:50:33.809208Z"
        },
        "trusted": true,
        "id": "1QTaBP-WRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Data(Dataset):\n",
        "    def __init__(self, english_sentences, telugu_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.telugu_sentences = telugu_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return self.english_sentences[ind], self.telugu_sentences[ind]\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.812592Z",
          "iopub.execute_input": "2024-03-19T03:50:33.813492Z",
          "iopub.status.idle": "2024-03-19T03:50:33.821202Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.813447Z",
          "shell.execute_reply": "2024-03-19T03:50:33.820036Z"
        },
        "trusted": true,
        "id": "UqNrfm5URI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = Data(english_sen_tokensied, telugu_sen_tokensied)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.823135Z",
          "iopub.execute_input": "2024-03-19T03:50:33.82367Z",
          "iopub.status.idle": "2024-03-19T03:50:33.8345Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.823625Z",
          "shell.execute_reply": "2024-03-19T03:50:33.833341Z"
        },
        "trusted": true,
        "id": "9XKMIcKPRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.836227Z",
          "iopub.execute_input": "2024-03-19T03:50:33.837399Z",
          "iopub.status.idle": "2024-03-19T03:50:33.85083Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.83733Z",
          "shell.execute_reply": "2024-03-19T03:50:33.849494Z"
        },
        "trusted": true,
        "id": "cI5Zpe9HRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.852403Z",
          "iopub.execute_input": "2024-03-19T03:50:33.853067Z",
          "iopub.status.idle": "2024-03-19T03:50:33.866209Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.853031Z",
          "shell.execute_reply": "2024-03-19T03:50:33.864802Z"
        },
        "trusted": true,
        "id": "BgydvhC2RI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(data_set, BATCH_SIZE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.868087Z",
          "iopub.execute_input": "2024-03-19T03:50:33.868986Z",
          "iopub.status.idle": "2024-03-19T03:50:33.875154Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.868939Z",
          "shell.execute_reply": "2024-03-19T03:50:33.873902Z"
        },
        "trusted": true,
        "id": "sBWc6ZslRI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ind, (english, telugu) in enumerate(train_loader):\n",
        "    print(english)\n",
        "    print(telugu)\n",
        "    print(english.size())\n",
        "    print(telugu.size())\n",
        "    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.876731Z",
          "iopub.execute_input": "2024-03-19T03:50:33.877254Z",
          "iopub.status.idle": "2024-03-19T03:50:33.922178Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.87721Z",
          "shell.execute_reply": "2024-03-19T03:50:33.921231Z"
        },
        "trusted": true,
        "id": "Xb_Q-WNsRI4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_INF = -1e9\n",
        "\n",
        "# we need 2 masks : padding_mask [ not to update weights bcz of padding token], lookahead_mask [not to look in future]\n",
        "def create_masks(eng_batch, tel_batch):\n",
        "    batch = len(eng_batch)\n",
        "    look_ahead_mask = torch.triu(torch.full([MAX_SEQ_LEN, MAX_SEQ_LEN], True), diagonal = 1) # jisko nehi dekna tha - usko True pai set kiye\n",
        "    encoder_padding_mask = torch.full([batch, MAX_SEQ_LEN, MAX_SEQ_LEN], True) # all intilised to true\n",
        "    decoder_padding_mask_self_attention = torch.full([batch, MAX_SEQ_LEN, MAX_SEQ_LEN], True)\n",
        "    decoder_padding_mask_cross_attention = torch.full([batch, MAX_SEQ_LEN,  MAX_SEQ_LEN], True)\n",
        "\n",
        "\n",
        "    for i in range(batch):\n",
        "        eng_len, tel_len = len(eng_batch[i]), len(tel_batch[i])\n",
        "        encoder_padding_mask[i, :eng_len, :eng_len] = False\n",
        "        decoder_padding_mask_self_attention[i, :tel_len, :tel_len] = False\n",
        "        decoder_padding_mask_cross_attention[i , :tel_len, :tel_len] = False\n",
        "\n",
        "    encoder_padding_mask = torch.where(encoder_padding_mask, NEG_INF, 0)\n",
        "    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INF, 0)\n",
        "    decoder_padding_mask_cross_attention = torch.where(decoder_padding_mask_cross_attention, NEG_INF, 0)\n",
        "\n",
        "    return encoder_padding_mask, decoder_self_attention_mask, decoder_padding_mask_cross_attention\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.923858Z",
          "iopub.execute_input": "2024-03-19T03:50:33.924209Z",
          "iopub.status.idle": "2024-03-19T03:50:33.93476Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.92418Z",
          "shell.execute_reply": "2024-03-19T03:50:33.93339Z"
        },
        "trusted": true,
        "id": "07n51sX3RI4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "look = torch.triu(torch.full([20, 20],True), diagonal = 1)\n",
        "print(look)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.936348Z",
          "iopub.execute_input": "2024-03-19T03:50:33.936943Z",
          "iopub.status.idle": "2024-03-19T03:50:33.959217Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.93691Z",
          "shell.execute_reply": "2024-03-19T03:50:33.958417Z"
        },
        "trusted": true,
        "id": "MuxISmohRI4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = torch.full([2, 5, 5], True)\n",
        "print(e)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.960688Z",
          "iopub.execute_input": "2024-03-19T03:50:33.961294Z",
          "iopub.status.idle": "2024-03-19T03:50:33.967669Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.961262Z",
          "shell.execute_reply": "2024-03-19T03:50:33.966482Z"
        },
        "trusted": true,
        "id": "LM98BbuGRI4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e[0,:3,:3] = False\n",
        "e[1, :3, :3] = False\n",
        "print(e)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.969747Z",
          "iopub.execute_input": "2024-03-19T03:50:33.970798Z",
          "iopub.status.idle": "2024-03-19T03:50:33.98481Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.970756Z",
          "shell.execute_reply": "2024-03-19T03:50:33.983488Z"
        },
        "trusted": true,
        "id": "oTOxEWJtRI4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL START**"
      ],
      "metadata": {
        "id": "Cw20ZWkwRI4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob = 0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p = drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x -> batch*seq*d_model\n",
        "        x = self.linear1(x) # batch*seq*hidden\n",
        "        x = self.relu(x) # batch*seq*hidden\n",
        "        x = self.dropout(x) # batch*seq*hidden\n",
        "        x = self.linear2(x) # batch*seq*d_model\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameter_shape, eps = 1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        # tells about along which dimension we want to perform layer normalization\n",
        "        self.parameter_shape = parameter_shape # typically it is embedding dimension [d_model]\n",
        "        self.eps = eps\n",
        "        # gamma, beta --> learnable parameters\n",
        "        self.gamma = nn.Parameter(torch.ones(parameter_shape)) # [d_model] --> standard deviation\n",
        "        self.beta = nn.Parameter(torch.ones(parameter_shape)) # [d_model] --> mean\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs --> batch*seq*d_model\n",
        "        dims = [-(i + 1) for i in range(len(self.parameter_shape))] # boils to -1\n",
        "        # if keepdim = Flase --> batch*seq\n",
        "        mean = inputs.mean(dim = dims, keepdim = True) # batch*seq*1\n",
        "        var = ((inputs - mean) ** 2).mean(dim = dims, keepdim = True) # batch*seq*1\n",
        "        std = (var + self.eps).sqrt() # batch*seq*d_model\n",
        "        # iske wazah se hoga --> mean = 0, standDev = 1\n",
        "        y = (inputs - mean) / std # ( DOUBT : yaha pai dim mention karne ki zarurat nehi hain kya)\n",
        "        # as said earlier for masking --> torch adds it for every batch & seq --> BUT DO CHECK ONCE\n",
        "        out = self.gamma * y + self.beta # batch*seq*d_model\n",
        "        # we have 512 learnable parameters in gamma & beta --> same as broadcasting discussed earlier\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask):\n",
        "    d_k = q.size()[-1] #head_dim\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # batch*head*seq*head_dim , batch*head*head*_dim*seq, batch*head*seq*seq\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "#         scaled += mask # mask -->batch * seq * seq --> pytorch is good enough to add it across every batch\n",
        "    attention = f.softmax(scaled, dim = -1)\n",
        "    values = torch.matmul(attention, v) #batch*head*seq*seq , batch*head*seq*head_dim\n",
        "\n",
        "    return values, attention\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model//num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model, 3*d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, seq_length, d_model = x.size() # batch*seq*d_model\n",
        "        qkv = self.qkv_layer(x) #batch*seq*d_model*3\n",
        "        # here --> every seq : 8 heads : q, k, v\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim) # batch*seq*noOfHead*(3*head_dim)\n",
        "        # it would be better to have --> every head: all seq : q, k ,v\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim = -1) # q, k, v ==> batch*head*seq*head_dim\n",
        "        # values : batch*head*seq*head_dim, attention : batch*head*seq*seq\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, seq_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "\n",
        "        return out # batch*seq*d_model ==> same as X, bt with more context aware\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq):\n",
        "        super().__init__()\n",
        "        self.max_seq_length = max_seq\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_seq_length)\n",
        "                          .reshape(self.max_seq_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "\n",
        "class SentenceEmbeddings(nn.Module):\n",
        "    def __init__(self, max_seq_len, d_model, start_token, end_token, pad_token, language_to_index):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.d_model = d_model\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.pad_token = pad_token\n",
        "        self.lan_to_index = language_to_index\n",
        "        self.vocab = len(language_to_index)\n",
        "\n",
        "        print(self.vocab, self.d_model)\n",
        "        self.embed = nn.Embedding(self.vocab, self.d_model)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "    def batch_tokenize(self, batch):\n",
        "        def tokeniser(sen):\n",
        "            sen_to_ind = [self.lan_to_index[x] for x in sen]\n",
        "#             sen_to_ind.insert(0, lan_to_index[START_TOKEN])\n",
        "            sen_to_ind.append(self.lan_to_index[END_TOKEN])\n",
        "            for _ in range(len(sen_to_ind), MAX_SEQ_LEN):\n",
        "                sen_to_ind.append(self.lan_to_index[PADDING_TOKEN])\n",
        "            return torch.tensor(sen_to_ind)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append(tokeniser(batch[sentence_num]))\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x - [batch, length of sentence]\n",
        "#         print(x.size(), \"out\")\n",
        "        x = self.batch_tokenize(x) # [batch, max_seq]\n",
        "#         print(x.size(), \"after\")\n",
        "        x = self.embed(x) # [batch, max_seq, d_model]\n",
        "        pos = self.position_encoder()\n",
        "        x = self.dropout(x + pos) # [batch, max_seq, d_model]\n",
        "        return x # [batch, max_seq, d_model]\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
        "        self.norm1 = LayerNormalization(parameter_shape = [d_model])\n",
        "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model = d_model, hidden = ffn_hidden, drop_prob = drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameter_shape = [d_model])\n",
        "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        # x --> batch*seq*d_model\n",
        "        residual_x = x.clone() # batch*seq*d_model\n",
        "        x = self.attention(x, mask = self_attention_mask) # batch*seq*d_model\n",
        "        x = self.dropout1(x) # batch * seq * d_model\n",
        "        x = self.norm1(x + residual_x) # batch * seq * d_model\n",
        "\n",
        "        residual_x = x.clone() # batch*seq*d_model\n",
        "        x = self.ffn(x) # batch*seq*d_model\n",
        "        x = self.dropout2(x) # batch*seq*d_model\n",
        "        x = self.norm2(x + residual_x) # batch*seq*d_model\n",
        "        return x # batch*seq*d_model --> same as of input, bt much more context aware\n",
        "\n",
        "\n",
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask = inputs\n",
        "        # as used Sequential every layer is stored as dict[name is _modules] {name:layer}\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x #[batch, seqLen, d_model]\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, drop_prob, d_model, ffn_hidden, num_heads, num_layers, max_seq, en_to_ind, start_token, end_token, pad_token):\n",
        "        super().__init__()\n",
        "        self.sentenceEmbedding = SentenceEmbeddings(max_seq, d_model, start_token, end_token, pad_token, en_to_ind)\n",
        "        # nn.Sequential takes only one argument as we have x & mask we need to write out own forwad function\n",
        "        self.layers =  SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        # x -> [batch, seqLen]\n",
        "        x = self.sentenceEmbedding(x) # [batch, max_seq, d_model]\n",
        "        x = self.layers(x,self_attention_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CrossMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, y, cross_attention_mask):\n",
        "        batch_size, seq_length, d_model = x.size() # batch*seq*d_model\n",
        "        kv = self.kv_layer(x) #batch*seq*d_model*3\n",
        "        q = self.q_layer(y)\n",
        "        # here --> every seq : 8 heads : q, k, v\n",
        "        kv = kv.reshape(batch_size, seq_length, self.num_heads, 2*self.head_dim) # batch*seq*noOfHead*(3*head_dim)\n",
        "        q = q.reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim = -1) # k, v ==> batch*head*seq*head_dim\n",
        "        # values : batch*head*seq*head_dim, attention : batch*head*seq*seq\n",
        "        values, attention = scaled_dot_product(q, k, v, cross_attention_mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, seq_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "\n",
        "        return out # batch*seq*d_model ==> same as X, bt with more context aware\n",
        "\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
        "        self.norm1 = LayerNormalization(parameter_shape = [d_model])\n",
        "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
        "\n",
        "        self.cross_attention = CrossMultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
        "        self.norm2 = LayerNormalization(parameter_shape = [d_model])\n",
        "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model = d_model, hidden = ffn_hidden, drop_prob = drop_prob)\n",
        "        self.norm3 = LayerNormalization(parameter_shape = [d_model])\n",
        "        self.dropout3 = nn.Dropout(p = drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        # x, y -> [batch, maxSeq, d_model], self & cross _attenion --> [batch, seq, seq]\n",
        "        residual_y = y.clone() # [batch, maxSeq, d_model]\n",
        "        y = self.attention(y, self_attention_mask) # [batch, maxSeq, d_model]\n",
        "        y = self.dropout1(y) # [batch, maxSeq, d_model]\n",
        "        y = self.norm1(y + residual_y) # [batch, maxSeq, d_model]\n",
        "\n",
        "        residual_y = y.clone() # [batch, maxSeq, d_model]\n",
        "        y = self.cross_attention(x, y, cross_attention_mask) # [batch, maxSeq, d_model]\n",
        "        y = self.dropout2(y) # [batch, maxSeq, d_model]\n",
        "        y = self.norm2(y + residual_y) # [batch, maxSeq, d_model]\n",
        "\n",
        "        residual_y = y\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y) # [batch, maxSeq, d_model]\n",
        "        y = self.norm3(y + residual_y) # [batch, maxSeq, d_model]\n",
        "\n",
        "        return y # [batch, maxSeq, d_model] --> vectors having context of input of decoder with the knowledge gained from encoder\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, drop_prob, d_model, ffn_hidden, num_heads, num_layers, max_seq, te_to_ind, start_token, end_token, pad_token):\n",
        "        super().__init__()\n",
        "        self.sentenceEmbedding = SentenceEmbeddings(max_seq, d_model, start_token, end_token, pad_token, te_to_ind)\n",
        "        # nn.Sequential takes only one argument as we have x & mask we need to write out own forwad function\n",
        "        self.layers =  SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        # x -> [batch, max_seq, d_model], y -> [batch, seqLen]\n",
        "        y = self.sentenceEmbedding(y) # [batch, max_seq, d_model]\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, drop_prob, d_model, ffn_hidden, num_heads, no_layers, max_seq, en_to_ind, te_to_ind, te_vocab_size,START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(drop_prob, d_model, ffn_hidden, num_heads, no_layers, max_seq, en_to_ind, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(drop_prob, d_model, ffn_hidden, num_heads, no_layers, max_seq, te_to_ind, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, te_vocab_size)\n",
        "\n",
        "    def forward(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask):\n",
        "        # x, y ==> [batch, senLength]\n",
        "        x = self.encoder(x, encoder_self_attention_mask) # [batch, maxSeq, d_model]\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
        "        out = self.linear(out)\n",
        "        return out # [batch, max_seq, te_vocab_size]\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:33.991404Z",
          "iopub.execute_input": "2024-03-19T03:50:33.992165Z",
          "iopub.status.idle": "2024-03-19T03:50:34.062425Z",
          "shell.execute_reply.started": "2024-03-19T03:50:33.992122Z",
          "shell.execute_reply": "2024-03-19T03:50:34.06081Z"
        },
        "trusted": true,
        "id": "Tn2UZ9HZRI4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = data_set = Data(english__sentences, telugu__sentences)\n",
        "data_batched = DataLoader(data_set, BATCH_SIZE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:34.063973Z",
          "iopub.execute_input": "2024-03-19T03:50:34.06543Z",
          "iopub.status.idle": "2024-03-19T03:50:34.078883Z",
          "shell.execute_reply.started": "2024-03-19T03:50:34.065344Z",
          "shell.execute_reply": "2024-03-19T03:50:34.077308Z"
        },
        "trusted": true,
        "id": "X3Bfx64pRI4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for eng, tel in train_loader:\n",
        "    print(eng)\n",
        "    print(tel)\n",
        "    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:34.081704Z",
          "iopub.execute_input": "2024-03-19T03:50:34.082287Z",
          "iopub.status.idle": "2024-03-19T03:50:34.093271Z",
          "shell.execute_reply.started": "2024-03-19T03:50:34.082243Z",
          "shell.execute_reply": "2024-03-19T03:50:34.092364Z"
        },
        "trusted": true,
        "id": "C5pHjqFNRI4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(0.1, 512, 1024, 8, 5, 200,english_to_index, telugu_to_index, len(telugu_vocab),START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=telugu_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "learning_rate = 0.1\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:34.094474Z",
          "iopub.execute_input": "2024-03-19T03:50:34.095244Z",
          "iopub.status.idle": "2024-03-19T03:50:37.744985Z",
          "shell.execute_reply.started": "2024-03-19T03:50:34.095207Z",
          "shell.execute_reply": "2024-03-19T03:50:37.743815Z"
        },
        "trusted": true,
        "id": "WGdtIfTgRI4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "for i in range(EPOCHS):\n",
        "    for eng, tel in data_batched:\n",
        "        encoder_padding_mask, decoder_self_attention_mask, decoder_padding_mask_cross_attention = create_masks(eng, tel)\n",
        "        # [batch, max_seq, te_vocab_size]\n",
        "        output = model.forward(eng, tel, encoder_padding_mask, decoder_self_attention_mask, decoder_padding_mask_cross_attention)\n",
        "        labels = model.decoder.sentenceEmbedding.batch_tokenize(tel)  # [batch, max_seq]\n",
        "        loss = criterion(\n",
        "            output.view(-1, len(telugu_to_index)),\n",
        "            labels.view(-1)\n",
        "        )\n",
        "        valid_indicies = torch.where(labels.view(-1) == telugu_to_index[PADDING_TOKEN], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optimizer.zero_grad()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        text = \"అనఘ నా సోదరి\"\n",
        "        en_sample = [\"Anagha is my sister\"]\n",
        "        te_sample = [\"<START>\"]\n",
        "        for j in range(200):\n",
        "            encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(en_sample, te_sample)\n",
        "            output_sam = model.forward(eng, tel, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
        "            next_token_prob_distribution = output_sam[0][j]\n",
        "            next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "            next_token = index_to_telugu[next_token_index]\n",
        "            te_sample = [te_sample[0] + next_token, ]\n",
        "            if next_token == END_TOKEN:\n",
        "                break\n",
        "        print(te_sample)\n",
        "#         break\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-19T03:50:37.746838Z",
          "iopub.execute_input": "2024-03-19T03:50:37.747571Z",
          "iopub.status.idle": "2024-03-19T04:00:09.905648Z",
          "shell.execute_reply.started": "2024-03-19T03:50:37.747526Z",
          "shell.execute_reply": "2024-03-19T04:00:09.903708Z"
        },
        "trusted": true,
        "id": "b8x_DXljRI4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZGHOmt9PRI4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}